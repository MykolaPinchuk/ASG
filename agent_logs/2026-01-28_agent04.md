# 2026-01-28 — agent04

## Goal for this cycle

Improve OSS-model “success” end-to-end:
- Fewer `providerErrors` / pass turns / malformed outputs.
- Better gameplay performance vs built-in baselines (`GreedyBot`, `MixBot`).

Hard constraint respected throughout: **no strategy hints/advice in prompts** (mechanics explanations and derived bookkeeping aids are allowed).

## What we changed (code)

### Mechanics-only prompt + reliability hardening (OpenAI-compatible provider)

File: `src/providers/openaiCompat.ts`

- System prompt clarifications (mechanics-only):
  - Actions apply sequentially; enables multi-edge move chains within a ply.
  - Reinforce then move in the same ply is legal (due to sequential actions).
  - Income per ply includes `baseIncome + sum(supplyYield owned)`.
  - Combat resolution details, plus an optional exact win-prob computation note (noise is uniform over integers in `[-n..+n]`).
- “Bookkeeping aids” added to the prompt payload (not advice):
  - `supplyNodes`: explicit list of nodes with `supplyYield > 0` + owners.
  - `distances`: BFS distance from every node to each HQ (`toMyHq`, `toEnemyHq`).
- Schema tightening: tool schema requires `actions` to be non-empty (`minItems: 1`).
- OSS reasoning-model defaults: increased `timeoutMs` and `max_tokens` (to reduce near-timeout failures and “budget-empty” missing-JSON behavior).
- Retry budgeting: enforce overall deadline across retries so the agent server doesn’t time out after multiple upstream attempts.

### Agent-server robustness

File: `src/cli/agentServer.ts`

- Tolerate case/whitespace variations in `move.from`/`move.to` via normalization to known node ids (reduces “invalid move → pass” outcomes).

### Eval harness defaults for reasoning models

Files:
- `src/cli/evalModelsVsMix.ts`
- `src/cli/agentVsRandom.ts`
- `src/cli/sweepOssModels.ts`

- Increased default timeouts / max tokens for “thinking/reasoning” model ids.

### Replay + latency instrumentation (this last user-driven change)

User requirement: **ALWAYS save replays** and **latency must always be present** (some controllers omit `latencyMs`).

Files:
- `src/game/match.ts`: always records `latencyMs` for each ply (fallback to measured wall-clock around `controller.decide()` if the controller doesn’t provide it).
- `src/cli/checkDeterminism.ts`: determinism normalization now zeros out `latencyMs` so determinism checks remain meaningful.
- `src/cli/evalModelsVsMix.ts` + `src/cli/agentVsRandom.ts`: treat `--save-replays=false` as ignored (always save) and print latency stats for all turns as well as “ok-only” turns.

## Experiments + results

### Important evaluation metric alignment

The user clarified the core bar for usefulness:
- **15 turns** = `turnCapPlies=30`. If a model can’t win within this horizon, it’s considered “no good”.

Earlier in this cycle we ran some `turnCapPlies=60` evaluations for stability/longer-horizon assessment; later runs used the short horizon.

### Short-horizon (15 turns) results — already run

Scenario: `scenarios/scenario_01.json` (Two Lanes, Two Resources)  
Seeds: 3–7 (`--games 5 --seed-start 3`)  

- `chutes / tngtech/DeepSeek-R1T-Chimera`:
  - vs `GreedyBot`: **5W-0D-0L** (wins at plies 25/9/5/9/13), `providerErrors=0`, `passTurns=0`
  - vs `MixBot`: **4W-1D-0L** (draw is turn-cap), `providerErrors=0`, `passTurns=0`
- `chutes / deepseek-ai/DeepSeek-V3-0324-TEE`:
  - vs `GreedyBot`: **1W-4D-0L**, `providerErrors=0`, `passTurns=0`
  - vs `MixBot`: **4W-1D-0L**, one game had `providerErrors=1` and `passTurns=1` (provider instability)

Conclusion: models other than NanoGPT `deepseek/deepseek-v3.2` can clear the short-horizon bar; **Chimera is decisively strong** in this scenario.

### Selected longer-horizon results (not the primary bar)

- `nanogpt / deepseek/deepseek-v3.2` vs `GreedyBot`:
  - `turnCapPlies=30` small sample (seeds 3–5): 3/3 draws
  - `turnCapPlies=60` (seeds 3–5): 2W/1D/0L
- `nanogpt / mistralai/devstral-2-123b-instruct-2512` vs `GreedyBot`, `turnCapPlies=60` (seeds 3–7): 0W/2D/3L with multiple pass turns.

These helped separate “capability” (planning/strength) from “reliability” (timeouts/invalid outputs).

## What we learned (high signal)

1) **Reliability improvements are highly leverageable** for OSS:
   - Small increases in timeout and max tokens materially reduce `providerErrors` and “budget-empty” missing-final-JSON failures.
   - Tightening schemas (`actions` non-empty) + node-id normalization reduces avoidable invalid actions.

2) **Mechanics-only scaffolding helps weaker models without violating the no-strategy constraint**:
   - Supplying `supplyNodes` + HQ distances reduces bookkeeping burden and correlates with stronger play in some OSS models.

3) **Short-horizon strength varies widely**:
   - Chimera is decisively above the bar in `scenario_01`.
   - Some models are “non-losing” but not decisive vs Greedy at 15 turns (draw-heavy).

4) **Replay hygiene matters for debugging**:
   - Prior runs used `--save-replays=false`, so those specific replays cannot be recovered without reruns.
   - Going forward, replays are always saved and include latency data per ply.

## Artifacts / where to look

- Short-horizon writeup: `docs/diagnostics/2026-01-29_agent04_oss_short_horizon_eval.md`
- Running log: `agent_logs/current.md`

## Suggested next steps for a fresh agent

- Evaluate additional OSS candidates at **`turnCapPlies=30`** (15 turns), `--games 5`, saving replays:
  - `chutesai/Mistral-Small-3.1-24B-Instruct-2503` (baseline candidate that wasn’t completed due to interrupt)
  - Any other provider baselines in `configs/oss_baselines.json`
- If any “latency missing” still appears, inspect which controller path produced it; with the `runMatch` fallback, replay `turns[*].latencyMs` should always be present.

