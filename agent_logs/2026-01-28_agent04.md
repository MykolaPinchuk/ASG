# 2026-01-28 — agent04

## Goal for this cycle

Improve OSS-model “success” end-to-end:
- Fewer `providerErrors` / pass turns / malformed outputs.
- Better gameplay performance vs built-in baselines (`GreedyBot`, `MixBot`).

Hard constraint respected throughout: **no strategy hints/advice in prompts** (mechanics explanations and derived bookkeeping aids are allowed).

## What we changed (code)

### Mechanics-only prompt + reliability hardening (OpenAI-compatible provider)

File: `src/providers/openaiCompat.ts`

- System prompt clarifications (mechanics-only):
  - Actions apply sequentially; enables multi-edge move chains within a ply.
  - Reinforce then move in the same ply is legal (due to sequential actions).
  - Income per ply includes `baseIncome + sum(supplyYield owned)`.
  - Combat resolution details, plus an optional exact win-prob computation note (noise is uniform over integers in `[-n..+n]`).
- “Bookkeeping aids” added to the prompt payload (not advice):
  - `supplyNodes`: explicit list of nodes with `supplyYield > 0` + owners.
  - `distances`: BFS distance from every node to each HQ (`toMyHq`, `toEnemyHq`).
- Schema tightening: tool schema requires `actions` to be non-empty (`minItems: 1`).
- OSS reasoning-model defaults: increased `timeoutMs` and `max_tokens` (to reduce near-timeout failures and “budget-empty” missing-JSON behavior).
- Retry budgeting: enforce overall deadline across retries so the agent server doesn’t time out after multiple upstream attempts.

### Agent-server robustness

File: `src/cli/agentServer.ts`

- Tolerate case/whitespace variations in `move.from`/`move.to` via normalization to known node ids (reduces “invalid move → pass” outcomes).

### Eval harness defaults for reasoning models

Files:
- `src/cli/evalModelsVsMix.ts`
- `src/cli/agentVsRandom.ts`
- `src/cli/sweepOssModels.ts`

- Increased default timeouts / max tokens for “thinking/reasoning” model ids.

### Replay + latency instrumentation (this last user-driven change)

User requirement: **ALWAYS save replays** and **latency must always be present** (some controllers omit `latencyMs`).

Files:
- `src/game/match.ts`: always records `latencyMs` for each ply (fallback to measured wall-clock around `controller.decide()` if the controller doesn’t provide it).
- `src/cli/checkDeterminism.ts`: determinism normalization now zeros out `latencyMs` so determinism checks remain meaningful.
- `src/cli/evalModelsVsMix.ts` + `src/cli/agentVsRandom.ts`: treat `--save-replays=false` as ignored (always save) and print latency stats for all turns as well as “ok-only” turns.

## Experiments + results

### Important evaluation metric alignment

The user clarified the core bar for usefulness:
- **15 turns** = `turnCapPlies=30`. If a model can’t win within this horizon, it’s considered “no good”.

Earlier in this cycle we ran some `turnCapPlies=60` evaluations for stability/longer-horizon assessment; later runs used the short horizon.

### Short-horizon (15 turns) results — already run

Scenario: `scenarios/scenario_01.json` (Two Lanes, Two Resources)  
Seeds: 3–7 (`--games 5 --seed-start 3`)  

- `chutes / tngtech/DeepSeek-R1T-Chimera`:
  - vs `GreedyBot`: **5W-0D-0L** (wins at plies 25/9/5/9/13), `providerErrors=0`, `passTurns=0`
  - vs `MixBot`: **4W-1D-0L** (draw is turn-cap), `providerErrors=0`, `passTurns=0`
- `chutes / deepseek-ai/DeepSeek-V3-0324-TEE`:
  - vs `GreedyBot`: **1W-4D-0L**, `providerErrors=0`, `passTurns=0`
  - vs `MixBot`: **4W-1D-0L**, one game had `providerErrors=1` and `passTurns=1` (provider instability)

Conclusion: models other than NanoGPT `deepseek/deepseek-v3.2` can clear the short-horizon bar; **Chimera is decisively strong** in this scenario.

### Selected longer-horizon results (not the primary bar)

- `nanogpt / deepseek/deepseek-v3.2` vs `GreedyBot`:
  - `turnCapPlies=30` small sample (seeds 3–5): 3/3 draws
  - `turnCapPlies=60` (seeds 3–5): 2W/1D/0L
- `nanogpt / mistralai/devstral-2-123b-instruct-2512` vs `GreedyBot`, `turnCapPlies=60` (seeds 3–7): 0W/2D/3L with multiple pass turns.

These helped separate “capability” (planning/strength) from “reliability” (timeouts/invalid outputs).

## What we learned (high signal)

1) **Reliability improvements are highly leverageable** for OSS:
   - Small increases in timeout and max tokens materially reduce `providerErrors` and “budget-empty” missing-final-JSON failures.
   - Tightening schemas (`actions` non-empty) + node-id normalization reduces avoidable invalid actions.

2) **Mechanics-only scaffolding helps weaker models without violating the no-strategy constraint**:
   - Supplying `supplyNodes` + HQ distances reduces bookkeeping burden and correlates with stronger play in some OSS models.

3) **Short-horizon strength varies widely**:
   - Chimera is decisively above the bar in `scenario_01`.
   - Some models are “non-losing” but not decisive vs Greedy at 15 turns (draw-heavy).

4) **Replay hygiene matters for debugging**:
   - Prior runs used `--save-replays=false`, so those specific replays cannot be recovered without reruns.
   - Going forward, replays are always saved and include latency data per ply.

## Artifacts / where to look

- Short-horizon writeup: `docs/diagnostics/2026-01-29_agent04_oss_short_horizon_eval.md`
- Running log: `agent_logs/current.md`

## Suggested next steps for a fresh agent

- Evaluate additional OSS candidates at **`turnCapPlies=30`** (15 turns), `--games 5`, saving replays:
  - `chutesai/Mistral-Small-3.1-24B-Instruct-2503` (baseline candidate that wasn’t completed due to interrupt)
  - Any other provider baselines in `configs/oss_baselines.json`
- If any “latency missing” still appears, inspect which controller path produced it; with the `runMatch` fallback, replay `turns[*].latencyMs` should always be present.

## Raw cycle timestamps (from `agent_logs/current.md`)

- 2026-01-28 05:28:54 PST — Onboarded (read state/spec and a bounded set of hot-path files; quick smoke: `npm run typecheck`, `npm run check:determinism -- --seed 1 --p1 greedy --p2 random`). Next: pick one v0.5 task from `HANDOFF.md` (draw-rate tuning sweep, viewer match summary panel, or persisting per-turn agent diagnostics into replay + viewer).
- 2026-01-28 05:51:40 PST — OSS reliability small tweak: in `agent:eval-vs-mix` + `agent:vs-random`, default OpenAI-compatible upstream timeout is now **65s** for “thinking/reasoning” model ids (still 60s for non-reasoning) to reduce timeout-driven `providerErrors`. Smoke (NanoGPT, `turnCapPlies=2`, 1 game each, no explicit `--timeout-ms`): `qwen/qwen3-next-80b-a3b-thinking`, `deepseek/deepseek-v3.2:thinking`, `deepseek-ai/deepseek-v3.2-exp-thinking` all returned non-pass legal actions with `providerErrors=0`.
- 2026-01-28 13:11:37 PST — Continued OSS reliability hardening: (1) OpenAI tool schema now requires `actions` `minItems=1`; (2) agent server now tolerates case/whitespace variations in move `from`/`to` via case-insensitive node-id normalization; (3) bumped default upstream timeout for reasoning models to **80s** (and client `--agent-timeout-ms` default to 95s in eval CLIs) to reduce near-timeout failures; (4) bumped default `max_tokens` for reasoning models to 800 in `openai_compat` to reduce “budget-empty”/missing-final-JSON behavior.
- 2026-01-28 13:52:06 PST — Prompt tweak (mechanics-only): clarified that owning `supplyYield>0` nodes increases income, and softened the “think-hint” wording to allow spending time to think (still requiring JSON-only output).
- 2026-01-28 13:52:53 PST — Prompt tweak (mechanics-only): clarified that sequential actions allow “reinforce then move” within the same ply (helps agents exploit action budget without any strategy hints).
- 2026-01-28 13:57:15 PST — Prompt tweak (mechanics-only): added map distances to each HQ and an explicit `supplyNodes` list (derived from scenario state) to reduce per-turn bookkeeping load on weaker OSS models without prescribing any strategy.
- 2026-01-28 14:10:34 PST — Quick validation: NanoGPT `deepseek/deepseek-v3.2` vs `GreedyBot` at `turnCapPlies=30` now drew 3/3 (seeds 3–5) with `passTurns=0` and `providerErrors=0` after adding `distances`+`supplyNodes` to the prompt; still no wins in that small sample. Added a mechanics-only note on computing exact combat win probability from the noise distribution.
- 2026-01-28 15:01:50 PST — Stronger perf check: NanoGPT `deepseek/deepseek-v3.2` vs `GreedyBot` at `turnCapPlies=60` went 2W/1D/0L (seeds 3–5) with `passTurns=0` and `providerErrors=0`.
- 2026-01-28 17:03:13 PST — Short-horizon evals (`turnCapPlies=30`): Chutes `tngtech/DeepSeek-R1T-Chimera` beat `GreedyBot` 5/5 and `MixBot` 4W/1D (seeds 3–7). Chutes `deepseek-ai/DeepSeek-V3-0324-TEE` went 1W/4D vs `GreedyBot` and 4W/1D vs `MixBot` (one run had a single provider-error turn). Instrumentation hardening: always record per-turn `latencyMs` in replays (fallback to measured wall-clock), normalize latency in determinism check, and force eval CLIs to always save replays. Details: `docs/diagnostics/2026-01-29_agent04_oss_short_horizon_eval.md`.
